{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "from archetypes.datasets import make_archetypal_dataset\n",
    "import numpy as np\n",
    "from time import time\n",
    "from archetypes.algorithms import BiAA\n",
    "from archetypes.algorithms.torch import BiAA as BiAA_nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataframe = None\n",
    "for n_elements in [100, 500, 1000, 5000, 10000]:\n",
    "    print(n_elements)\n",
    "    for n_archetypes_i in [3, 5, 10, 20]:\n",
    "        print(n_archetypes_i)\n",
    "        shape = (int(np.sqrt(n_elements)), int(np.sqrt(n_elements)))\n",
    "\n",
    "        # check that shape[0] > n_archetypes[0] and shape[1] > n_archetypes[1]:\n",
    "        if shape[0] < n_archetypes_i or shape[1] < n_archetypes_i:\n",
    "            continue\n",
    "\n",
    "        n_archetypes = (n_archetypes_i, n_archetypes_i)\n",
    "        archetypes = np.random.uniform(0, 1, n_archetypes)\n",
    "\n",
    "        data, _ = make_archetypal_dataset(archetypes, shape, noise=0.1, generator=n_elements)\n",
    "        data = data.astype(np.float32)\n",
    "\n",
    "        start = time()\n",
    "        model_biaa = BiAA(n_archetypes, max_iter=2_000, tol=0)\n",
    "        model_biaa.fit(data)\n",
    "        stop = time()\n",
    "\n",
    "        biaa_time = stop - start\n",
    "\n",
    "        data_torch = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "        start = time()\n",
    "        model_biaa_nn = BiAA_nn(n_archetypes, *shape, device=\"cpu\")\n",
    "        model_biaa_nn.train(data_torch, 2_000, learning_rate=0.05)\n",
    "        stop = time()\n",
    "\n",
    "        biaa_nn_time = stop - start\n",
    "\n",
    "        start = time()\n",
    "        model_biaa_nn = BiAA_nn(n_archetypes, *shape, device=\"cuda\")\n",
    "        model_biaa_nn.train(data_torch, 2_000, learning_rate=0.05)\n",
    "        stop = time()\n",
    "\n",
    "        biaa_nn_cuda_time = stop - start\n",
    "\n",
    "        # create a dataframe with n_elements, n_archetypes, model, device, time\n",
    "        dataframe_i = pd.DataFrame({\n",
    "            \"n_elements\": [n_elements],\n",
    "            \"n_archetypes\": [n_archetypes_i],\n",
    "            \"model\": [\"BiAA\"],\n",
    "            \"device\": [\"cpu\"],\n",
    "            \"time\": [biaa_time]\n",
    "        })\n",
    "\n",
    "        dataframe_i = pd.concat([dataframe_i, pd.DataFrame({\n",
    "            \"n_elements\": [n_elements],\n",
    "            \"n_archetypes\": [n_archetypes_i],\n",
    "            \"model\": [\"BiAA (gradient-based)\"],\n",
    "            \"device\": [\"cpu\"],\n",
    "            \"time\": [biaa_nn_time]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        dataframe_i = pd.concat([dataframe_i, pd.DataFrame({\n",
    "            \"n_elements\": [n_elements],\n",
    "            \"n_archetypes\": [n_archetypes_i],\n",
    "            \"model\": [\"BiAA (gradient-based)\"],\n",
    "            \"device\": [\"cuda\"],\n",
    "            \"time\": [biaa_nn_cuda_time]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        dataframe = pd.concat([dataframe, dataframe_i], ignore_index=True)\n",
    "\n",
    "        # save dataframe\n",
    "\n",
    "        dataframe.to_csv(\"performance_results.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
